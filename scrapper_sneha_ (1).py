# -*- coding: utf-8 -*-
"""scrapper_sneha_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UdJsjYX4E8rrkX2gpZQ1neaPp0gGz-Xo
"""

!pip install requests beautifulsoup4

base_url = "https://www.health.harvard.edu"



import requests
from bs4 import BeautifulSoup

URL = "https://www.health.harvard.edu/health-a-to-z"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

response = requests.get(URL, headers=headers)


print("Status Code:", response.status_code)

soup = BeautifulSoup(response.text, "html.parser")

all_links = soup.select("main a")

conditions = []

for link in all_links:
    href = link.get("href", "")
    text = link.get_text(strip=True)

    if href.startswith("/a-to-z/") and text:
        conditions.append(text)


for cond in conditions:
    print(cond)

all_links = soup.select("main a")

condition_links = []

for link in all_links:
    href = link.get("href", "")
    text = link.get_text(strip=True)

    if href.startswith("/a-to-z/") and text:
        full_url = base_url + href
        condition_links.append((text, full_url))


condition_links[:5]

condition_data = []

for name, url in condition_links[:]:
    try:
        page = requests.get(url, headers=headers)
        page_soup = BeautifulSoup(page.text, "html.parser")

        # Extract main content text
        main_content = page_soup.select_one("main")
        if main_content:
            text = main_content.get_text(separator="\n", strip=True)
            condition_data.append((name, url, text))
            print(f"✅ Scraped: {name}")
        else:
            print(f"⚠️ No content for: {name}")

    except Exception as e:
        print(f"❌ Error for {name}: {e}")

import json

def parse_condition_page(name, url):
    try:
        page = requests.get(url, headers=headers)
        soup = BeautifulSoup(page.text, "html.parser")

        main = soup.select_one("main")
        if not main:
            return None

        content = {}
        current_section = "Overview"
        content[current_section] = ""

        for tag in main.find_all(["h2", "h3", "p"]):
            if tag.name in ["h2", "h3"]:
                current_section = tag.get_text(strip=True)
                content[current_section] = ""
            elif tag.name == "p":
                content[current_section] += tag.get_text(strip=True) + "\n"

        return {
            "name": name,
            "url": url,
            "content": content
        }

    except Exception as e:
        print(f"❌ Failed to parse {name}: {e}")
        return None

disease_json_data = []

for name, url in condition_links[:]:  # use [:] for all
    print(f"⏳ Scraping: {name}")
    result = parse_condition_page(name, url)
    if result:
        disease_json_data.append(result)

with open("harvard_conditions_sneha.json", "w", encoding='utf-8') as f:
    json.dump(disease_json_data, f, indent=2, ensure_ascii=False)

from google.colab import files
files.download("harvard_conditions_sneha.json")

import json

# Load your scraped JSON file
with open("harvard_conditions_sneha.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# Count the number of diseases
num_diseases = len(data)
print(f"✅ Total diseases scraped: {num_diseases}")

